# fast-api-deploy
Production-Ready ML Inference API with CI/CD Deployed a containerized FastAPI-based ML inference service integrating a pre-trained model. Implemented structured logging, input validation, Docker containerization, and CI pipeline using GitHub Actions for automated testing and deployment.
